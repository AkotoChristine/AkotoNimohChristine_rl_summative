# -*- coding: utf-8 -*-
"""a2c-training.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qEoKRoBDd-2_RoidLx4qAKU3KO7jGk9T
"""

!pip install stable-baselines3[extra] gymnasium pybullet shimmy -q

!pip install stable-baselines3[extra]
!pip install "autorom[accept-rom-license]"

!pip install numpy==1.26.4 --force-reinstall --no-cache-dir

import sys
sys.path.append('/kaggle/input/firerescueeenv')

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from stable_baselines3 import A2C
from stable_baselines3.common.evaluation import evaluate_policy
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.vec_env import DummyVecEnv
import gymnasium as gym

from custom_env import FireRescueEnv

# Cell 3: Hyperparameter Configurations
a2c_configs = [
    # Config 1: Baseline
    {
        'name': 'A2C_Baseline',
        'learning_rate': 7e-4,
        'n_steps': 5,
        'gamma': 0.99,
        'gae_lambda': 1.0,
        'ent_coef': 0.0,
        'vf_coef': 0.5,
        'max_grad_norm': 0.5,
        'use_rms_prop': True
    },
    # Config 2: High Learning Rate
    {
        'name': 'A2C_HighLR',
        'learning_rate': 1e-3,
        'n_steps': 5,
        'gamma': 0.99,
        'gae_lambda': 1.0,
        'ent_coef': 0.0,
        'vf_coef': 0.5,
        'max_grad_norm': 0.5,
        'use_rms_prop': True
    },
    # Config 3: Low Learning Rate
    {
        'name': 'A2C_LowLR',
        'learning_rate': 1e-4,
        'n_steps': 5,
        'gamma': 0.99,
        'gae_lambda': 1.0,
        'ent_coef': 0.0,
        'vf_coef': 0.5,
        'max_grad_norm': 0.5,
        'use_rms_prop': True
    },
    # Config 4: More Steps
    {
        'name': 'A2C_MoreSteps',
        'learning_rate': 7e-4,
        'n_steps': 20,
        'gamma': 0.99,
        'gae_lambda': 1.0,
        'ent_coef': 0.0,
        'vf_coef': 0.5,
        'max_grad_norm': 0.5,
        'use_rms_prop': True
    },
    # Config 5: Fewer Steps
    {
        'name': 'A2C_FewerSteps',
        'learning_rate': 7e-4,
        'n_steps': 2,
        'gamma': 0.99,
        'gae_lambda': 1.0,
        'ent_coef': 0.0,
        'vf_coef': 0.5,
        'max_grad_norm': 0.5,
        'use_rms_prop': True
    },
    # Config 6: High Entropy Coefficient (More Exploration)
    {
        'name': 'A2C_HighEntropy',
        'learning_rate': 7e-4,
        'n_steps': 5,
        'gamma': 0.99,
        'gae_lambda': 1.0,
        'ent_coef': 0.01,
        'vf_coef': 0.5,
        'max_grad_norm': 0.5,
        'use_rms_prop': True
    },
    # Config 7: GAE Lambda = 0.95
    {
        'name': 'A2C_GAE95',
        'learning_rate': 7e-4,
        'n_steps': 5,
        'gamma': 0.99,
        'gae_lambda': 0.95,
        'ent_coef': 0.0,
        'vf_coef': 0.5,
        'max_grad_norm': 0.5,
        'use_rms_prop': True
    },
    # Config 8: High Value Function Coefficient
    {
        'name': 'A2C_HighVFCoef',
        'learning_rate': 7e-4,
        'n_steps': 5,
        'gamma': 0.99,
        'gae_lambda': 1.0,
        'ent_coef': 0.0,
        'vf_coef': 1.0,
        'max_grad_norm': 0.5,
        'use_rms_prop': True
    },
    # Config 9: Low Gamma
    {
        'name': 'A2C_LowGamma',
        'learning_rate': 7e-4,
        'n_steps': 5,
        'gamma': 0.95,
        'gae_lambda': 1.0,
        'ent_coef': 0.0,
        'vf_coef': 0.5,
        'max_grad_norm': 0.5,
        'use_rms_prop': True
    },
    # Config 10: High Gradient Clipping
    {
        'name': 'A2C_HighGradClip',
        'learning_rate': 7e-4,
        'n_steps': 5,
        'gamma': 0.99,
        'gae_lambda': 1.0,
        'ent_coef': 0.0,
        'vf_coef': 0.5,
        'max_grad_norm': 1.0,
        'use_rms_prop': True
    }
]

def train_and_evaluate_a2c(config, total_timesteps=100_000):
    print(f"\n{'='*60}")
    print(f" Training: {config['name']}")
    print(f"{'='*60}")

    # ---------------------------- CREATE ENV ----------------------------
    def make_env():
        return FireRescueEnv(render_mode=None, max_steps=500)

    env = DummyVecEnv([make_env])

    # ---------------------------- BUILD MODEL ----------------------------
    model = A2C(
        "MlpPolicy",
        env,
        learning_rate=config['learning_rate'],
        n_steps=config['n_steps'],
        gamma=config['gamma'],
        gae_lambda=config['gae_lambda'],
        ent_coef=config['ent_coef'],
        vf_coef=config['vf_coef'],
        max_grad_norm=config['max_grad_norm'],
        verbose=1,
        tensorboard_log=f"./a2c_tensorboard/{config['name']}/",
        device="auto"
    )

    # ---------------------------- TRAIN ----------------------------
    model.learn(total_timesteps=total_timesteps)

    # ---------------------------- SAVE MODEL ----------------------------
    os.makedirs("./a2c_models", exist_ok=True)
    model_path = f"./a2c_models/{config['name']}.zip"
    model.save(model_path)
    print(f"\n✓ Model saved to: {model_path}")

    # ---------------------------- EVALUATE ----------------------------
    eval_env = FireRescueEnv(render_mode=None, max_steps=500)

    episode_rewards = []

    for _ in range(10):  # 10 eval episodes
        obs, _ = eval_env.reset()
        obs = np.array(obs, dtype=np.float32)  # <-- FIX #1
        done = False
        ep_reward = 0

        while not done:
            action, _ = model.predict(obs, deterministic=True)
            action = int(action)  # <-- FIX #2 (SB3 returns ndarray)

            obs, reward, terminated, truncated, _ = eval_env.step(action)

            obs = np.array(obs, dtype=np.float32)  # <-- FIX #3
            ep_reward += reward

            done = terminated or truncated

        episode_rewards.append(ep_reward)

    eval_env.close()

    # Compute stats
    mean_reward = float(np.mean(episode_rewards))
    std_reward = float(np.std(episode_rewards))

    print(f"\n✓ Evaluation: Mean Reward = {mean_reward:.2f} ± {std_reward:.2f}")

    return {
        "name": config["name"],
        "config": config,
        "mean_reward": mean_reward,
        "std_reward": std_reward,
        "model": model
    }

import os

# ---------------------------- RUN ALL CONFIGS ----------------------------
results = []

for i, config in enumerate(a2c_configs, start=1):
    print(f"\n>>> Running configuration {i}/{len(a2c_configs)}")
    try:
        result = train_and_evaluate_a2c(config, total_timesteps=5000)
        results.append(result)
    except Exception as e:
        print(f"⚠ Skipped {config['name']} due to error: {e}")

print("\nAll A2C configs finished.")

# Cell 6: Create Results DataFrame
results_df = pd.DataFrame([
    {
        'Configuration': r['name'],
        'Mean Reward': r['mean_reward'],
        'Std Reward': r['std_reward'],
        'Learning Rate': r['config']['learning_rate'],
        'N Steps': r['config']['n_steps'],
        'Gamma': r['config']['gamma'],
        'Entropy Coef': r['config']['ent_coef'],
        'VF Coef': r['config']['vf_coef']
    }
    for r in results
])

# Cell 7: Visualization - Bar Plot
plt.figure(figsize=(14, 6))
plt.bar(results_df['Configuration'], results_df['Mean Reward'],
        yerr=results_df['Std Reward'], capsize=5, alpha=0.7, color='forestgreen')
plt.xlabel('Configuration', fontsize=12, fontweight='bold')
plt.ylabel('Mean Reward', fontsize=12, fontweight='bold')
plt.title('A2C Performance Across Different Hyperparameter Configurations',
          fontsize=14, fontweight='bold')
plt.xticks(rotation=45, ha='right')
plt.grid(axis='y', alpha=0.3)
plt.tight_layout()
plt.savefig('a2c_results_comparison.png', dpi=300)
plt.show()

# Cell 8: Hyperparameter Impact Analysis
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# Learning Rate Impact
lr_data = results_df[results_df['Configuration'].str.contains('LR|Baseline')]
axes[0, 0].bar(lr_data['Configuration'], lr_data['Mean Reward'],
               yerr=lr_data['Std Reward'], capsize=5, alpha=0.7, color='coral')
axes[0, 0].set_title('Learning Rate Impact', fontweight='bold')
axes[0, 0].set_ylabel('Mean Reward')
axes[0, 0].tick_params(axis='x', rotation=45)
axes[0, 0].grid(axis='y', alpha=0.3)

# N Steps Impact
steps_data = results_df[results_df['Configuration'].str.contains('Steps|Baseline')]
axes[0, 1].bar(steps_data['Configuration'], steps_data['Mean Reward'],
               yerr=steps_data['Std Reward'], capsize=5, alpha=0.7, color='lightgreen')
axes[0, 1].set_title('N Steps Impact', fontweight='bold')
axes[0, 1].set_ylabel('Mean Reward')
axes[0, 1].tick_params(axis='x', rotation=45)
axes[0, 1].grid(axis='y', alpha=0.3)

# Entropy Coefficient Impact
entropy_data = results_df[results_df['Configuration'].str.contains('Entropy|Baseline')]
axes[1, 0].bar(entropy_data['Configuration'], entropy_data['Mean Reward'],
               yerr=entropy_data['Std Reward'], capsize=5, alpha=0.7, color='mediumpurple')
axes[1, 0].set_title('Entropy Coefficient Impact', fontweight='bold')
axes[1, 0].set_ylabel('Mean Reward')
axes[1, 0].tick_params(axis='x', rotation=45)
axes[1, 0].grid(axis='y', alpha=0.3)

# Value Function Coefficient Impact
vf_data = results_df[results_df['Configuration'].str.contains('VFCoef|Baseline')]
axes[1, 1].bar(vf_data['Configuration'], vf_data['Mean Reward'],
               yerr=vf_data['Std Reward'], capsize=5, alpha=0.7, color='gold')
axes[1, 1].set_title('Value Function Coefficient Impact', fontweight='bold')
axes[1, 1].set_ylabel('Mean Reward')
axes[1, 1].tick_params(axis='x', rotation=45)
axes[1, 1].grid(axis='y', alpha=0.3)

plt.tight_layout()
plt.savefig('a2c_hyperparameter_analysis.png', dpi=300)
plt.show()

# Cell 9: Best Model Selection
best_result = max(results, key=lambda x: x['mean_reward'])
print(f"\n BEST CONFIGURATION: {best_result['name']}")
print(f"Mean Reward: {best_result['mean_reward']:.2f} +/- {best_result['std_reward']:.2f}")
print("\nBest Hyperparameters:")
for key, value in best_result['config'].items():
    if key != 'name':
        print(f"  {key}: {value}")

# Cell 10: Save Results
results_df.to_csv('a2c_results.csv', index=False)
print("\n Results saved to 'a2c_results.csv'")
print(" Models saved in './models/' directory")
print(" Tensorboard logs saved in './a2c_tensorboard/' directory")

best_model = A2C.load(f"./a2c_models/{best_result['name']}")

!zip -r a2c_logs.zip a2c_logs/

# Cell 11: Load and Test Best Model (Optional)

# To test the best model visually:
from fire_rescue_env_discrete import FireRescueEnvDiscrete
from stable_baselines3 import A2C

# Load the best model
best_model = A2C.load(f"./models/{best_result['name']}")

# Create environment with rendering
test_env = FireRescueEnvDiscrete(render_mode="human", headless=False)
obs, _ = test_env.reset()

for _ in range(1000):
    action, _ = best_model.predict(obs, deterministic=True)
    obs, reward, terminated, truncated, _ = test_env.step(action)
    if terminated or truncated:
        obs, _ = test_env.reset()

test_env.close()
"""

