# -*- coding: utf-8 -*-
"""ppo-training.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FInwX7h7rj_gZslvxFhmw1EXSsNzzwhC
"""

!pip install stable-baselines3[extra] gymnasium pybullet shimmy -q

!pip install stable-baselines3[extra]
!pip install "autorom[accept-rom-license]"

!pip install stable-baselines3[extra] gymnasium pybullet shimmy -q

import pandas as pd

list = [2,2,2,22,2,22,2,2,2,22,2,2]

pd.Series(list).to_csv("my_list.csv", index=False)

import sys
sys.path.append('/kaggle/input/firerescueenv')  # Update with your dataset name

from custom_env import FireRescueEnv

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from stable_baselines3 import PPO
from stable_baselines3.common.evaluation import evaluate_policy
from stable_baselines3.common.monitor import Monitor
import gymnasium as gym

ppo_configs = [
    # Config 1: Baseline
    {
        'name': 'PPO_Baseline',
        'learning_rate': 3e-4,
        'n_steps': 2048,
        'batch_size': 64,
        'n_epochs': 10,
        'gamma': 0.99,
        'gae_lambda': 0.95,
        'clip_range': 0.2,
        'ent_coef': 0.0,
        'vf_coef': 0.5,
        'max_grad_norm': 0.5
    },
    # Config 2: High Learning Rate
    {
        'name': 'PPO_HighLR',
        'learning_rate': 1e-3,
        'n_steps': 2048,
        'batch_size': 64,
        'n_epochs': 10,
        'gamma': 0.99,
        'gae_lambda': 0.95,
        'clip_range': 0.2,
        'ent_coef': 0.0,
        'vf_coef': 0.5,
        'max_grad_norm': 0.5
    },
    # Config 3: Low Learning Rate
    {
        'name': 'PPO_LowLR',
        'learning_rate': 1e-4,
        'n_steps': 2048,
        'batch_size': 64,
        'n_epochs': 10,
        'gamma': 0.99,
        'gae_lambda': 0.95,
        'clip_range': 0.2,
        'ent_coef': 0.0,
        'vf_coef': 0.5,
        'max_grad_norm': 0.5
    },
    # Config 4: Large Batch Size
    {
        'name': 'PPO_LargeBatch',
        'learning_rate': 3e-4,
        'n_steps': 2048,
        'batch_size': 256,
        'n_epochs': 10,
        'gamma': 0.99,
        'gae_lambda': 0.95,
        'clip_range': 0.2,
        'ent_coef': 0.0,
        'vf_coef': 0.5,
        'max_grad_norm': 0.5
    },
    # Config 5: Small Batch Size
    {
        'name': 'PPO_SmallBatch',
        'learning_rate': 3e-4,
        'n_steps': 2048,
        'batch_size': 32,
        'n_epochs': 10,
        'gamma': 0.99,
        'gae_lambda': 0.95,
        'clip_range': 0.2,
        'ent_coef': 0.0,
        'vf_coef': 0.5,
        'max_grad_norm': 0.5
    },
    # Config 6: More Epochs
    {
        'name': 'PPO_MoreEpochs',
        'learning_rate': 3e-4,
        'n_steps': 2048,
        'batch_size': 64,
        'n_epochs': 20,
        'gamma': 0.99,
        'gae_lambda': 0.95,
        'clip_range': 0.2,
        'ent_coef': 0.0,
        'vf_coef': 0.5,
        'max_grad_norm': 0.5
    },
    # Config 7: Fewer Epochs
    {
        'name': 'PPO_FewerEpochs',
        'learning_rate': 3e-4,
        'n_steps': 2048,
        'batch_size': 64,
        'n_epochs': 5,
        'gamma': 0.99,
        'gae_lambda': 0.95,
        'clip_range': 0.2,
        'ent_coef': 0.0,
        'vf_coef': 0.5,
        'max_grad_norm': 0.5
    },
    # Config 8: High Entropy (More Exploration)
    {
        'name': 'PPO_HighEntropy',
        'learning_rate': 3e-4,
        'n_steps': 2048,
        'batch_size': 64,
        'n_epochs': 10,
        'gamma': 0.99,
        'gae_lambda': 0.95,
        'clip_range': 0.2,
        'ent_coef': 0.01,
        'vf_coef': 0.5,
        'max_grad_norm': 0.5
    },
    # Config 9: Large Clip Range
    {
        'name': 'PPO_LargeClip',
        'learning_rate': 3e-4,
        'n_steps': 2048,
        'batch_size': 64,
        'n_epochs': 10,
        'gamma': 0.99,
        'gae_lambda': 0.95,
        'clip_range': 0.3,
        'ent_coef': 0.0,
        'vf_coef': 0.5,
        'max_grad_norm': 0.5
    },
    # Config 10: Small Clip Range
    {
        'name': 'PPO_SmallClip',
        'learning_rate': 3e-4,
        'n_steps': 2048,
        'batch_size': 64,
        'n_epochs': 10,
        'gamma': 0.99,
        'gae_lambda': 0.95,
        'clip_range': 0.1,
        'ent_coef': 0.0,
        'vf_coef': 0.5,
        'max_grad_norm': 0.5
    }
]

# ============================================================================
# CELL: Training Function for PPO
# ============================================================================

def train_and_evaluate_ppo(config, total_timesteps=100000):
    """Train and evaluate a PPO model"""

    print(f"\n{'='*60}")
    print(f"Training: {config['name']}")
    print(f"{'='*60}")

    # Create environment
    env = FireRescueEnv(render_mode=None, max_steps=500)

    # Create model
    model = PPO(
        "MlpPolicy",
        env,
        learning_rate=config['learning_rate'],
        n_steps=config['n_steps'],
        batch_size=config['batch_size'],
        n_epochs=config['n_epochs'],
        gamma=config['gamma'],
        gae_lambda=config['gae_lambda'],
        clip_range=config['clip_range'],
        ent_coef=config['ent_coef'],
        verbose=1,
        tensorboard_log=f"./ppo_tensorboard/{config['name']}/",
        device='auto'
    )

    # Train
    try:
        model.learn(total_timesteps=total_timesteps, progress_bar=True)
    except Exception as e:
        print(f"⚠️ Training error: {e}")
        env.close()
        return {
            'name': config['name'],
            'config': config,
            'error': str(e)
        }

    # Evaluate
    eval_rewards = []
    for _ in range(10):
        obs, _ = env.reset()
        episode_reward = 0
        done = False

        while not done:
            action, _ = model.predict(obs, deterministic=True)
            obs, reward, terminated, truncated, _ = env.step(action)
            episode_reward += reward
            done = terminated or truncated

        eval_rewards.append(episode_reward)

    mean_reward = float(np.mean(eval_rewards))
    std_reward = float(np.std(eval_rewards))

    print(f"\n✓ Training complete!")
    print(f"  Mean Reward: {mean_reward:.2f} ± {std_reward:.2f}")

    env.close()

    return {
        'name': config['name'],
        'config': config,
        'mean_reward': mean_reward,
        'std_reward': std_reward,
        'model': model
    }

results = []
for i, config in enumerate(ppo_configs, 1):
    print(f"\n Training Configuration {i}/10")
    result = train_and_evaluate_ppo(config, total_timesteps=100000)
    results.append(result)

results = []

for i, config in enumerate(dqn_configs, 1):
    print(f"\n Training Configuration {i}/10")
    result = train_and_evaluate_dqn(config, total_timesteps=100000)
    results.append(result)

    with open('/kaggle/working/dqn_results_intermediate.json', 'w') as f:
        json.dump(results, f, indent=2)

    print(f"\n✓ {config['name']} completed!")
    print(f"  Mean Reward: {result['mean_reward']:.2f} ± {result['std_reward']:.2f}")