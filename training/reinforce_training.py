# -*- coding: utf-8 -*-
"""reinforce_training.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dXu_YG8CbQlbs7Eg9QxdFwxhNZQ1rq4S
"""

!pip install stable-baselines3[extra]
!pip install "autorom[accept-rom-license]"

!pip install numpy==1.26.4 --force-reinstall --no-cache-dir

!pip install stable-baselines3[extra] gymnasium pybullet shimmy -q

import sys
sys.path.append('/kaggle/input/firerescueeenv')


import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.distributions import Categorical
import gymnasium as gym
from custom_env import FireRescueEnv

class PolicyNetwork(nn.Module):
    def __init__(self, obs_dim, action_dim, hidden_size=128):
        super(PolicyNetwork, self).__init__()
        self.fc1 = nn.Linear(obs_dim, hidden_size)
        self.fc2 = nn.Linear(hidden_size, hidden_size)
        self.fc3 = nn.Linear(hidden_size, action_dim)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = self.fc3(x)
        return torch.softmax(x, dim=-1)

class REINFORCEAgent:
    def __init__(self, obs_dim, action_dim, learning_rate=1e-3, gamma=0.99,
                 hidden_size=128, entropy_coef=0.0):
        self.policy = PolicyNetwork(obs_dim, action_dim, hidden_size)
        self.optimizer = optim.Adam(self.policy.parameters(), lr=learning_rate)
        self.gamma = gamma
        self.entropy_coef = entropy_coef

        self.saved_log_probs = []
        self.rewards = []
        self.entropies = []

    def select_action(self, state):
        state = torch.FloatTensor(state).unsqueeze(0)
        probs = self.policy(state)
        m = Categorical(probs)
        action = m.sample()

        self.saved_log_probs.append(m.log_prob(action))
        self.entropies.append(m.entropy())

        return action.item()

    def update(self):
        R = 0
        returns = []

        # Calculate discounted returns
        for r in self.rewards[::-1]:
            R = r + self.gamma * R
            returns.insert(0, R)

        returns = torch.tensor(returns)

        # Normalize returns
        if len(returns) > 1:
            returns = (returns - returns.mean()) / (returns.std() + 1e-8)

        policy_loss = []
        for log_prob, R, entropy in zip(self.saved_log_probs, returns, self.entropies):
            policy_loss.append(-log_prob * R - self.entropy_coef * entropy)

        self.optimizer.zero_grad()
        policy_loss = torch.stack(policy_loss).sum()
        policy_loss.backward()
        self.optimizer.step()

        # Clear memory
        self.saved_log_probs = []
        self.rewards = []
        self.entropies = []

        return policy_loss.item()

def train_reinforce(agent, env, num_episodes=500, max_steps=500):
    episode_rewards = []

    for episode in range(num_episodes):
        state, _ = env.reset()
        episode_reward = 0

        for step in range(max_steps):
            action = agent.select_action(state)
            next_state, reward, _, _, _ = env.step(action)  # ignore terminated/truncated
            agent.rewards.append(reward)
            episode_reward += reward
            state = next_state

        # Update policy after each episode
        loss = agent.update()
        episode_rewards.append(episode_reward)

        if (episode + 1) % 50 == 0:
            avg_reward = np.mean(episode_rewards[-50:])
            print(f"Episode {episode + 1}/{num_episodes} | Avg Reward: {avg_reward:.2f} | Loss: {loss:.2f}")

    return episode_rewards

def evaluate_reinforce(agent, env, num_episodes=10, max_steps=500):
    episode_rewards = []

    for _ in range(num_episodes):
        state, _ = env.reset()
        episode_reward = 0

        for step in range(max_steps):
            with torch.no_grad():
                state_tensor = torch.FloatTensor(state).unsqueeze(0)
                probs = agent.policy(state_tensor)
                action = torch.argmax(probs).item()

            state, reward, _, _, _ = env.step(action)  # ignore terminated/truncated
            episode_reward += reward

        episode_rewards.append(episode_reward)

    return np.mean(episode_rewards), np.std(episode_rewards)

reinforce_configs = [
    # Config 1: Baseline
    {
        'name': 'REINFORCE_Baseline',
        'learning_rate': 1e-3,
        'gamma': 0.99,
        'hidden_size': 128,
        'entropy_coef': 0.0
    },
    # Config 2: High Learning Rate
    {
        'name': 'REINFORCE_HighLR',
        'learning_rate': 5e-3,
        'gamma': 0.99,
        'hidden_size': 128,
        'entropy_coef': 0.0
    },
    # Config 3: Low Learning Rate
    {
        'name': 'REINFORCE_LowLR',
        'learning_rate': 1e-4,
        'gamma': 0.99,
        'hidden_size': 128,
        'entropy_coef': 0.0
    },
    # Config 4: High Gamma
    {
        'name': 'REINFORCE_HighGamma',
        'learning_rate': 1e-3,
        'gamma': 0.995,
        'hidden_size': 128,
        'entropy_coef': 0.0
    },
    # Config 5: Low Gamma
    {
        'name': 'REINFORCE_LowGamma',
        'learning_rate': 1e-3,
        'gamma': 0.95,
        'hidden_size': 128,
        'entropy_coef': 0.0
    },
    # Config 6: Large Network
    {
        'name': 'REINFORCE_LargeNet',
        'learning_rate': 1e-3,
        'gamma': 0.99,
        'hidden_size': 256,
        'entropy_coef': 0.0
    },
    # Config 7: Small Network
    {
        'name': 'REINFORCE_SmallNet',
        'learning_rate': 1e-3,
        'gamma': 0.99,
        'hidden_size': 64,
        'entropy_coef': 0.0
    },
    # Config 8: High Entropy (More Exploration)
    {
        'name': 'REINFORCE_HighEntropy',
        'learning_rate': 1e-3,
        'gamma': 0.99,
        'hidden_size': 128,
        'entropy_coef': 0.01
    },
    # Config 9: Very High Entropy
    {
        'name': 'REINFORCE_VeryHighEntropy',
        'learning_rate': 1e-3,
        'gamma': 0.99,
        'hidden_size': 128,
        'entropy_coef': 0.05
    },
    # Config 10: Balanced
    {
        'name': 'REINFORCE_Balanced',
        'learning_rate': 5e-4,
        'gamma': 0.98,
        'hidden_size': 128,
        'entropy_coef': 0.001
    }
]

results = []

num_episodes = 500
max_steps = 500  # force episode end

for i, config in enumerate(reinforce_configs, 1):
    print(f"\n{'='*60}")
    print(f"Training Configuration {i}/10: {config['name']}")
    print(f"{'='*60}")

    # Create environment
    env = FireRescueEnv(render_mode=None, max_steps=max_steps, headless=True)

    # Create agent
    agent = REINFORCEAgent(
        obs_dim=env.observation_space.shape[0],
        action_dim=env.action_space.n,
        learning_rate=config['learning_rate'],
        gamma=config['gamma'],
        hidden_size=config['hidden_size'],
        entropy_coef=config['entropy_coef']
    )

    # -------- Training Loop --------
    episode_rewards = []

    for episode in range(num_episodes):
        state, _ = env.reset()
        episode_reward = 0

        for step in range(max_steps):
            action = agent.select_action(state)
            next_state, reward, _, _, _ = env.step(action)  # ignore terminated/truncated
            agent.rewards.append(reward)
            episode_reward += reward
            state = next_state

        loss = agent.update()
        episode_rewards.append(episode_reward)

        if (episode + 1) % 50 == 0:
            avg_reward = np.mean(episode_rewards[-50:])
            print(f"Episode {episode+1}/{num_episodes} | Avg Reward: {avg_reward:.2f} | Loss: {loss:.2f}")

    # -------- Evaluation --------
    eval_env = FireRescueEnv(render_mode=None, max_steps=max_steps, headless=True)
    eval_rewards = []

    for _ in range(5):
        state, _ = eval_env.reset()
        episode_reward = 0

        for step in range(max_steps):
            with torch.no_grad():
                state_tensor = torch.FloatTensor(state).unsqueeze(0)
                probs = agent.policy(state_tensor)
                action = torch.argmax(probs).item()

            state, reward, _, _, _ = eval_env.step(action)
            episode_reward += reward

        eval_rewards.append(episode_reward)

    mean_reward = np.mean(eval_rewards)
    std_reward = np.std(eval_rewards)

    print(f"\n{config['name']} Results:")
    print(f"Mean Reward: {mean_reward:.2f} +/- {std_reward:.2f}")

    # Save model
    torch.save(agent.policy.state_dict(), f"./models/{config['name']}.pth")

    results.append({
        'name': config['name'],
        'config': config,
        'mean_reward': mean_reward,
        'std_reward': std_reward,
        'episode_rewards': episode_rewards,
        'agent': agent
    })

    env.close()
    eval_env.close()

# Cell 9: Create Results DataFrame
results_df = pd.DataFrame([
    {
        'Configuration': r['name'],
        'Mean Reward': r['mean_reward'],
        'Std Reward': r['std_reward'],
        'Learning Rate': r['config']['learning_rate'],
        'Gamma': r['config']['gamma'],
        'Hidden Size': r['config']['hidden_size'],
        'Entropy Coef': r['config']['entropy_coef']
    }
    for r in results
])

print("\n" + "="*80)
print("FINAL RESULTS SUMMARY")
print("="*80)
print(results_df.to_string(index=False))

# Cell 10: Visualization - Bar Plot
plt.figure(figsize=(14, 6))
plt.bar(results_df['Configuration'], results_df['Mean Reward'],
        yerr=results_df['Std Reward'], capsize=5, alpha=0.7, color='darkorange')
plt.xlabel('Configuration', fontsize=12, fontweight='bold')
plt.ylabel('Mean Reward', fontsize=12, fontweight='bold')
plt.title('REINFORCE Performance Across Different Hyperparameter Configurations',
          fontsize=14, fontweight='bold')
plt.xticks(rotation=45, ha='right')
plt.grid(axis='y', alpha=0.3)
plt.tight_layout()
plt.savefig('reinforce_results_comparison.png', dpi=300)
plt.show()

# Cell 11: Learning Curves
fig, axes = plt.subplots(2, 5, figsize=(20, 8))
axes = axes.flatten()

for i, result in enumerate(results):
    # Moving average
    window = 20
    rewards = result['episode_rewards']
    moving_avg = np.convolve(rewards, np.ones(window)/window, mode='valid')

    axes[i].plot(moving_avg, linewidth=2)
    axes[i].set_title(result['name'], fontweight='bold')
    axes[i].set_xlabel('Episode')
    axes[i].set_ylabel('Reward (MA-20)')
    axes[i].grid(alpha=0.3)

plt.tight_layout()
plt.savefig('reinforce_learning_curves.png', dpi=300)
plt.show()

# Cell 12: Hyperparameter Impact Analysis
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# Learning Rate Impact
lr_data = results_df[results_df['Configuration'].str.contains('LR|Baseline')]
axes[0, 0].bar(lr_data['Configuration'], lr_data['Mean Reward'],
               yerr=lr_data['Std Reward'], capsize=5, alpha=0.7, color='coral')
axes[0, 0].set_title('Learning Rate Impact', fontweight='bold')
axes[0, 0].set_ylabel('Mean Reward')
axes[0, 0].tick_params(axis='x', rotation=45)
axes[0, 0].grid(axis='y', alpha=0.3)

# Gamma Impact
gamma_data = results_df[results_df['Configuration'].str.contains('Gamma|Baseline')]
axes[0, 1].bar(gamma_data['Configuration'], gamma_data['Mean Reward'],
               yerr=gamma_data['Std Reward'], capsize=5, alpha=0.7, color='lightgreen')
axes[0, 1].set_title('Discount Factor (Gamma) Impact', fontweight='bold')
axes[0, 1].set_ylabel('Mean Reward')
axes[0, 1].tick_params(axis='x', rotation=45)
axes[0, 1].grid(axis='y', alpha=0.3)

# Network Size Impact
net_data = results_df[results_df['Configuration'].str.contains('Net|Baseline')]
axes[1, 0].bar(net_data['Configuration'], net_data['Mean Reward'],
               yerr=net_data['Std Reward'], capsize=5, alpha=0.7, color='mediumpurple')
axes[1, 0].set_title('Network Size Impact', fontweight='bold')
axes[1, 0].set_ylabel('Mean Reward')
axes[1, 0].tick_params(axis='x', rotation=45)
axes[1, 0].grid(axis='y', alpha=0.3)

# Entropy Impact
entropy_data = results_df[results_df['Configuration'].str.contains('Entropy|Baseline')]
axes[1, 1].bar(entropy_data['Configuration'], entropy_data['Mean Reward'],
               yerr=entropy_data['Std Reward'], capsize=5, alpha=0.7, color='gold')
axes[1, 1].set_title('Entropy Coefficient Impact', fontweight='bold')
axes[1, 1].set_ylabel('Mean Reward')
axes[1, 1].tick_params(axis='x', rotation=45)
axes[1, 1].grid(axis='y', alpha=0.3)

plt.tight_layout()
plt.savefig('reinforce_hyperparameter_analysis.png', dpi=300)
plt.show()

# Cell 13: Best Model Selection
best_result = max(results, key=lambda x: x['mean_reward'])
print(f"\n BEST CONFIGURATION: {best_result['name']}")
print(f"Mean Reward: {best_result['mean_reward']:.2f} +/- {best_result['std_reward']:.2f}")
print("\nBest Hyperparameters:")
for key, value in best_result['config'].items():
    if key != 'name':
        print(f"  {key}: {value}")

# Cell 14: Save Results
results_df.to_csv('reinforce_results.csv', index=False)
print("\n Results saved to 'reinforce_results.csv'")
print(" Models saved in './models/' directory")

# Cell 15: Load and Test Best Model (Optional)
"""
# To test the best model visually:
from fire_rescue_env_discrete import FireRescueEnvDiscrete
import torch

# Load the best model
best_agent = REINFORCEAgent(
    obs_dim=10,
    action_dim=9,
    learning_rate=best_result['config']['learning_rate'],
    gamma=best_result['config']['gamma'],
    hidden_size=best_result['config']['hidden_size']
)
best_agent.policy.load_state_dict(torch.load(f"./models/{best_result['name']}.pth"))

# Create environment with rendering
test_env = FireRescueEnvDiscrete(render_mode="human", headless=False)
obs, _ = test_env.reset()

for _ in range(1000):
    with torch.no_grad():
        state_tensor = torch.FloatTensor(obs).unsqueeze(0)
        probs = best_agent.policy(state_tensor)
        action = torch.argmax(probs).item()

    obs, reward, terminated, truncated, _ = test_env.step(action)
    if terminated or truncated:
        obs, _ = test_env.reset()

test_env.close()