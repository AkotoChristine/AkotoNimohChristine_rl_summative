# -*- coding: utf-8 -*-
"""dqn-training.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/124K-z3NK5R2tF6o3J8hBSOgLJZ03p44O
"""

!pip install stable-baselines3[extra] gymnasium pybullet shimmy -q

!pip install stable-baselines3[extra]
!pip install "autorom[accept-rom-license]"

!pip uninstall -y tensorboard tb-nightly tensorboard-plugin-wit

# Install compatible versions
!pip install tensorboard==2.14.0 protobuf==3.20.3 -q
!pip install stable-baselines3[extra] gymnasium pybullet shimmy -q

import sys
sys.path.append('/kaggle/input/firerescueeenv')
from custom_env import FireRescueEnv

# Cell 2: Imports
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from stable_baselines3 import DQN
from stable_baselines3.common.evaluation import evaluate_policy
from stable_baselines3.common.monitor import Monitor
import gymnasium as gym

# Cell 3: Hyperparameter Configurations
dqn_configs = [
    # Config 1: Baseline
    {
        'name': 'DQN_Baseline',
        'learning_rate': 1e-4,
        'buffer_size': 50000,
        'learning_starts': 1000,
        'batch_size': 32,
        'tau': 1.0,
        'gamma': 0.99,
        'target_update_interval': 1000,
        'exploration_fraction': 0.1,
        'exploration_initial_eps': 1.0,
        'exploration_final_eps': 0.05
    },
    # Config 2: High Learning Rate
    {
        'name': 'DQN_HighLR',
        'learning_rate': 5e-4,
        'buffer_size': 50000,
        'learning_starts': 1000,
        'batch_size': 32,
        'tau': 1.0,
        'gamma': 0.99,
        'target_update_interval': 1000,
        'exploration_fraction': 0.1,
        'exploration_initial_eps': 1.0,
        'exploration_final_eps': 0.05
    },
    # Config 3: Low Learning Rate
    {
        'name': 'DQN_LowLR',
        'learning_rate': 1e-5,
        'buffer_size': 50000,
        'learning_starts': 1000,
        'batch_size': 32,
        'tau': 1.0,
        'gamma': 0.99,
        'target_update_interval': 1000,
        'exploration_fraction': 0.1,
        'exploration_initial_eps': 1.0,
        'exploration_final_eps': 0.05
    },
    # Config 4: Large Batch Size
    {
        'name': 'DQN_LargeBatch',
        'learning_rate': 1e-4,
        'buffer_size': 100000,
        'learning_starts': 1000,
        'batch_size': 128,
        'tau': 1.0,
        'gamma': 0.99,
        'target_update_interval': 1000,
        'exploration_fraction': 0.1,
        'exploration_initial_eps': 1.0,
        'exploration_final_eps': 0.05
    },
    # Config 5: Small Batch Size
    {
        'name': 'DQN_SmallBatch',
        'learning_rate': 1e-4,
        'buffer_size': 50000,
        'learning_starts': 1000,
        'batch_size': 16,
        'tau': 1.0,
        'gamma': 0.99,
        'target_update_interval': 1000,
        'exploration_fraction': 0.1,
        'exploration_initial_eps': 1.0,
        'exploration_final_eps': 0.05
    },
    # Config 6: High Gamma (Future-focused)
    {
        'name': 'DQN_HighGamma',
        'learning_rate': 1e-4,
        'buffer_size': 50000,
        'learning_starts': 1000,
        'batch_size': 32,
        'tau': 1.0,
        'gamma': 0.995,
        'target_update_interval': 1000,
        'exploration_fraction': 0.1,
        'exploration_initial_eps': 1.0,
        'exploration_final_eps': 0.05
    },
    # Config 7: Low Gamma (Immediate rewards)
    {
        'name': 'DQN_LowGamma',
        'learning_rate': 1e-4,
        'buffer_size': 100000,
        'learning_starts': 1000,
        'batch_size': 32,
        'tau': 1.0,
        'gamma': 0.95,
        'target_update_interval': 1000,
        'exploration_fraction': 0.1,
        'exploration_initial_eps': 1.0,
        'exploration_final_eps': 0.05
    },
    # Config 8: Large Buffer
    {
        'name': 'DQN_LargeBuffer',
        'learning_rate': 1e-4,
        'buffer_size': 5000,
        'learning_starts': 5000,
        'batch_size': 32,
        'tau': 1.0,
        'gamma': 0.99,
        'target_update_interval': 1000,
        'exploration_fraction': 0.1,
        'exploration_initial_eps': 1.0,
        'exploration_final_eps': 0.05
    },
    # Config 9: Aggressive Exploration
    {
        'name': 'DQN_AggressiveExplore',
        'learning_rate': 1e-4,
        'buffer_size': 5000,
        'learning_starts': 1000,
        'batch_size': 32,
        'tau': 1.0,
        'gamma': 0.99,
        'target_update_interval': 1000,
        'exploration_fraction': 0.3,
        'exploration_initial_eps': 1.0,
        'exploration_final_eps': 0.1
    },
    # Config 10: Conservative Exploration
    {
        'name': 'DQN_ConservativeExplore',
        'learning_rate': 1e-4,
        'buffer_size': 100000,
        'learning_starts': 1000,
        'batch_size': 32,
        'tau': 1.0,
        'gamma': 0.99,
        'target_update_interval': 1000,
        'exploration_fraction': 0.05,
        'exploration_initial_eps': 1.0,
        'exploration_final_eps': 0.01
    }
]

# Cell 4: Training Function
def train_and_evaluate_dqn(config, total_timesteps=100000):
    """
    Train and evaluate a DQN agent with given configuration
    """
    print(f"\n{'='*60}")
    print(f"Training: {config['name']}")
    print(f"{'='*60}")

    # Create environment
    env = FireRescueEnv(render_mode=None, max_steps=500, headless=True)
    env = Monitor(env)

    # Create DQN model
    model = DQN(
        "MlpPolicy",
        env,
        learning_rate=config['learning_rate'],
        buffer_size=config['buffer_size'],
        learning_starts=config['learning_starts'],
        batch_size=config['batch_size'],
        tau=config['tau'],
        gamma=config['gamma'],
        target_update_interval=config['target_update_interval'],
        exploration_fraction=config['exploration_fraction'],
        exploration_initial_eps=config['exploration_initial_eps'],
        exploration_final_eps=config['exploration_final_eps'],
        verbose=1,
        tensorboard_log=f"./dqn_tensorboard/{config['name']}/"
    )

    # Train the model
    model.learn(total_timesteps=total_timesteps)

    # Evaluate the trained model
    eval_env = FireRescueEnv(render_mode=None, max_steps=500, headless=True)
    mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10)

    print(f"\n{config['name']} Results:")
    print(f"Mean Reward: {mean_reward:.2f} +/- {std_reward:.2f}")

    # Save the model
    model.save(f"./models/{config['name']}")

    env.close()
    eval_env.close()

    return {
        'name': config['name'],
        'config': config,
        'mean_reward': mean_reward,
        'std_reward': std_reward,
        'model': model
    }

# Cell 5: Train All Configurations
results = []
for i, config in enumerate(dqn_configs, 1):
    print(f"\n Training Configuration {i}/10")
    result = train_and_evaluate_dqn(config, total_timesteps=5000)
    results.append(result)

# Cell 6: Create Results DataFrame
results_df = pd.DataFrame([
    {
        'Configuration': r['name'],
        'Mean Reward': r['mean_reward'],
        'Std Reward': r['std_reward'],
        'Learning Rate': r['config']['learning_rate'],
        'Batch Size': r['config']['batch_size'],
        'Gamma': r['config']['gamma'],
        'Buffer Size': r['config']['buffer_size']
    }
    for r in results
])

print("\n" + "="*80)
print("FINAL RESULTS SUMMARY")
print("="*80)
print(results_df.to_string(index=False))

# Cell 7: Visualization - Bar Plot
plt.figure(figsize=(14, 6))
plt.bar(results_df['Configuration'], results_df['Mean Reward'],
        yerr=results_df['Std Reward'], capsize=5, alpha=0.7, color='steelblue')
plt.xlabel('Configuration', fontsize=12, fontweight='bold')
plt.ylabel('Mean Reward', fontsize=12, fontweight='bold')
plt.title('DQN Performance Across Different Hyperparameter Configurations',
          fontsize=14, fontweight='bold')
plt.xticks(rotation=45, ha='right')
plt.grid(axis='y', alpha=0.3)
plt.tight_layout()
plt.savefig('dqn_results_comparison.png', dpi=300)
plt.show()

# Cell 8: Hyperparameter Impact Analysis
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# Learning Rate Impact
lr_data = results_df[results_df['Configuration'].str.contains('LR|Baseline')]
axes[0, 0].bar(lr_data['Configuration'], lr_data['Mean Reward'],
               yerr=lr_data['Std Reward'], capsize=5, alpha=0.7, color='coral')
axes[0, 0].set_title('Learning Rate Impact', fontweight='bold')
axes[0, 0].set_ylabel('Mean Reward')
axes[0, 0].tick_params(axis='x', rotation=45)
axes[0, 0].grid(axis='y', alpha=0.3)

# Batch Size Impact
batch_data = results_df[results_df['Configuration'].str.contains('Batch|Baseline')]
axes[0, 1].bar(batch_data['Configuration'], batch_data['Mean Reward'],
               yerr=batch_data['Std Reward'], capsize=5, alpha=0.7, color='lightgreen')
axes[0, 1].set_title('Batch Size Impact', fontweight='bold')
axes[0, 1].set_ylabel('Mean Reward')
axes[0, 1].tick_params(axis='x', rotation=45)
axes[0, 1].grid(axis='y', alpha=0.3)

# Gamma Impact
gamma_data = results_df[results_df['Configuration'].str.contains('Gamma|Baseline')]
axes[1, 0].bar(gamma_data['Configuration'], gamma_data['Mean Reward'],
               yerr=gamma_data['Std Reward'], capsize=5, alpha=0.7, color='mediumpurple')
axes[1, 0].set_title('Discount Factor (Gamma) Impact', fontweight='bold')
axes[1, 0].set_ylabel('Mean Reward')
axes[1, 0].tick_params(axis='x', rotation=45)
axes[1, 0].grid(axis='y', alpha=0.3)

# Exploration Impact
explore_data = results_df[results_df['Configuration'].str.contains('Explore|Baseline')]
axes[1, 1].bar(explore_data['Configuration'], explore_data['Mean Reward'],
               yerr=explore_data['Std Reward'], capsize=5, alpha=0.7, color='gold')
axes[1, 1].set_title('Exploration Strategy Impact', fontweight='bold')
axes[1, 1].set_ylabel('Mean Reward')
axes[1, 1].tick_params(axis='x', rotation=45)
axes[1, 1].grid(axis='y', alpha=0.3)

plt.tight_layout()
plt.savefig('dqn_hyperparameter_analysis.png', dpi=300)
plt.show()

# Cell 9: Best Model Selection and Testing
best_result = max(results, key=lambda x: x['mean_reward'])
print(f"\n BEST CONFIGURATION: {best_result['name']}")
print(f"Mean Reward: {best_result['mean_reward']:.2f} +/- {best_result['std_reward']:.2f}")
print("\nBest Hyperparameters:")
for key, value in best_result['config'].items():
    if key != 'name':
        print(f"  {key}: {value}")

# Cell 10: Save Results
results_df.to_csv('dqn_results.csv', index=False)
print("\n Results saved to 'dqn_results.csv'")
print(" Models saved in './models/' directory")
print(" Tensorboard logs saved in './dqn_tensorboard/' directory")

# Cell 10: Save Results
results_df.to_csv('dqn_results.csv', index=False)
print("\n Results saved to 'dqn_results.csv'")
print(" Models saved in './models/' directory")
print(" Tensorboard logs saved in './dqn_tensorboard/' directory")

# Cell 11: Load and Test Best Model (Optional)
"""
# To test the best model visually:
from fire_rescue_env_discrete import FireRescueEnvDiscrete
from stable_baselines3 import DQN

# Load the best model
best_model = DQN.load(f"./models/{best_result['name']}")

# Create environment with rendering
test_env = FireRescueEnvDiscrete(render_mode="human", headless=False)
obs, _ = test_env.reset()

for _ in range(1000):
    action, _ = best_model.predict(obs, deterministic=True)
    obs, reward, terminated, truncated, _ = test_env.step(action)
    if terminated or truncated:
        obs, _ = test_env.reset()

test_env.close()
"""

